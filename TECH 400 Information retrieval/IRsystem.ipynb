{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2q-EZzXHTlT",
        "outputId": "445fe848-4db1-4a1d-be07-92d06900226b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvJa3-ppHfEM",
        "outputId": "a13843c3-88d4-4cc0-933b-d89ba60bf783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import logging\n",
        "import math\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_otJwmHSHq4i"
      },
      "outputs": [],
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "LEMMATIZER = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LM8BFG6lHvJe"
      },
      "outputs": [],
      "source": [
        "def load_text_files(folder_path):\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data[doc_id] = file.read()\n",
        "                doc_id_to_filename[doc_id] = filename\n",
        "                logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "                doc_id += 1\n",
        "    return data, doc_id_to_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IUfhf82oHyhS"
      },
      "outputs": [],
      "source": [
        "# Removes special characters from text, tokenizes it, eliminates stop words, and lemmatizes it.\n",
        "def tokenize(text):\n",
        "  tokens = text.lower()\n",
        "  text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
        "  tokens = text.split()\n",
        "  cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
        "  return cleaned_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gwhQPJjqH1bV"
      },
      "outputs": [],
      "source": [
        "def term_frequency(term, document):\n",
        "    return document.count(term) / len(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1DiP2NK_H56X"
      },
      "outputs": [],
      "source": [
        "def inverse_document_frequency(term, all_documents):\n",
        "    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "    return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jamwT19OH7ae"
      },
      "outputs": [],
      "source": [
        "def compute_tfidf(document, all_documents, vocab):\n",
        "    tfidf_vector = []\n",
        "    for term in vocab:\n",
        "        tf = term_frequency(term, document)\n",
        "        idf = inverse_document_frequency(term, all_documents)\n",
        "        tfidf_vector.append(tf * idf)\n",
        "    return np.array(tfidf_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qpc3uWCQH99O"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2) if norm_vec1 * norm_vec2 != 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbfkFFLpIAbC",
        "outputId": "ac5e5a65-ea5a-4043-c52d-1d06b67accbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 5 results for query: 'turbocharged'\n",
            "  Document: Genesis GV80.txt, Similarity: 0.0688\n",
            "  Document: Subaru Outback Wilderness.txt, Similarity: 0.0665\n",
            "  Document: Volvo XC90 Recharge.txt, Similarity: 0.0652\n",
            "  Document: Jeep Grand Cherokee 4xe.txt, Similarity: 0.0599\n",
            "  Document: Lexus RX 500h.txt, Similarity: 0.0573\n",
            "\n",
            "\n",
            "\n",
            "Top 5 results for query: 'eco-friendly hatchbacks'\n",
            "  Document: BMW iX.txt, Similarity: 0.0854\n",
            "  Document: Jeep Grand Cherokee 4xe.txt, Similarity: 0.0800\n",
            "  Document: Lexus RX 500h.txt, Similarity: 0.0766\n",
            "  Document: Tesla Model S Plaid.txt, Similarity: 0.0000\n",
            "  Document: Ford F-150 Lightning.txt, Similarity: 0.0000\n",
            "\n",
            "\n",
            "\n",
            "Top 5 results for query: 'hybrid vehicles'\n",
            "  Document: Volvo XC90 Recharge.txt, Similarity: 0.1356\n",
            "  Document: Jeep Grand Cherokee 4xe.txt, Similarity: 0.0903\n",
            "  Document: Lexus RX 500h.txt, Similarity: 0.0864\n",
            "  Document: BMW iX.txt, Similarity: 0.0732\n",
            "  Document: Porsche Taycan Turbo S.txt, Similarity: 0.0673\n",
            "\n",
            "\n",
            "\n",
            "Top 5 results for query: 'sports cars under $60k'\n",
            "  Document: Audi e-tron GT.txt, Similarity: 0.2389\n",
            "  Document: Porsche Taycan Turbo S.txt, Similarity: 0.1943\n",
            "  Document: Hyundai Ioniq 6.txt, Similarity: 0.0460\n",
            "  Document: Tesla Model S Plaid.txt, Similarity: 0.0366\n",
            "  Document: Ford F-150 Lightning.txt, Similarity: 0.0000\n",
            "\n",
            "\n",
            "Enter a query (or 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "def main():\n",
        "    folder_path = \"/content/drive/MyDrive/TECH 400 Information Retrieval/cars\"\n",
        "\n",
        "    # Load documents\n",
        "    docs, doc_id_to_filename = load_text_files(folder_path)\n",
        "\n",
        "    # Tokenize the documents\n",
        "    tokenized_docs = [tokenize(doc) for doc in docs.values()]\n",
        "\n",
        "    # Create vocabulary\n",
        "    vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "    logging.info(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "    # Compute TF-IDF vectors for each document\n",
        "    doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Start query processing loop\n",
        "    while True:\n",
        "        # Ask user for a query\n",
        "        query = input(\"Enter a query (or 'exit' to quit): \").strip()\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        tokenized_query = tokenize(query)\n",
        "        query_tfidf_vector = compute_tfidf(tokenized_query, tokenized_docs, vocab)\n",
        "\n",
        "        similarities = []\n",
        "\n",
        "        # Compute cosine similarity between the query and all documents\n",
        "        for doc_id, doc_vector in enumerate(doc_tfidf_vectors):\n",
        "            similarity = cosine_similarity(query_tfidf_vector, doc_vector)\n",
        "            similarities.append((doc_id, similarity))\n",
        "\n",
        "        # Sort the documents based on similarity in descending order\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Keep only the top 5 most similar documents\n",
        "        top_5_similarities = similarities[:5]\n",
        "\n",
        "        # Append the query and its top 5 results to the final results list\n",
        "        results.append((query, top_5_similarities))\n",
        "\n",
        "        # Display the results for the current query\n",
        "        print(f\"\\nTop 5 results for query: '{query}'\")\n",
        "        for doc_id, similarity in top_5_similarities:\n",
        "            filename = doc_id_to_filename[doc_id]\n",
        "            print(f\"  Document: {filename}, Similarity: {similarity:.4f}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    # Define the output path and filename\n",
        "    path = \"/content/drive/MyDrive/TECH 400 Information Retrieval/result\"\n",
        "    output_file = os.path.join(path, \"results_Nisha.txt\")\n",
        "\n",
        "    # Write results to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for query, similarities in results:\n",
        "            f.write(f\"Query: {query}\\n\")\n",
        "            for doc_id, similarity in similarities:\n",
        "                filename = doc_id_to_filename[doc_id]\n",
        "                f.write(f\"  Document: {filename}, Similarity: {similarity:.4f}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    logging.info(f\"Results written to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiYSiU2rJx3-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}